# EchoMind  
### The Human-Like, Offline AI Voice Assistant

**EchoMind** is a next-generation AI voice assistant that feels less like a chatbot and more like a real assistant.  
Built with a **local LLM (Phi-3 Mini)**, **Whisper STT**, **Edge TTS & Piper**, and a **multi-agent intelligence system**, EchoMind delivers fast, private, and emotionally aware conversations â€” fully offline.

> Talk naturally. Be remembered. Stay private.

---

## ğŸš€ Why EchoMind?

Most assistants are cloud-dependent, impersonal, and forgetful.  
EchoMind is different.

- âœ… **Offline-first**
- âœ… **Human-style conversation**
- âœ… **Persistent personal memory**
- âœ… **Multi-agent intelligence**
- âœ… **No data leaves your machine**

EchoMind is designed as a **personal AI operating system**, not just a voice bot.

---

## âœ¨ Core Capabilities

### ğŸ™ï¸ Natural Voice Interaction
- **Whisper** for accurate speech-to-text
- **Edge TTS & Piper** for expressive, low-latency speech
- Real conversational pacing, not robotic responses

### ğŸ§  Local Intelligence
- Powered by **Phi-3 Mini**
- Fast inference on consumer hardware
- Works without internet access

### ğŸ¤– Multi-Agent Architecture
Different agents handle different responsibilities:
- Conversation & reasoning
- Task execution (music, YouTube, system actions)
- Emotion & intent understanding
- Memory storage and recall
- Routing between local and external tools

This allows EchoMind to **think before it speaks**.

### ğŸ’¬ Human-Like Personality
- Prompt-engineered personality system
- Context-aware dialogue
- Emotion-sensitive responses
- Less â€œassistantâ€, more â€œcompanionâ€

### ğŸ§  Long-Term Personal Memory
- Remembers preferences, habits, and context
- Semantic recall using vector embeddings
- SQLite-backed persistent storage
- Learns how *you* like to interact

---

Designed for **scalability, modularity, and future SaaS expansion**.

---

## ğŸ§© How EchoMind Thinks

1. User speaks naturally  
2. Whisper converts speech â†’ text  
3. Intent & emotion are analyzed  
4. Multi-agent system decides next action  
5. Phi-3 generates a response  
6. Memory agent updates long-term context  
7. Edge TTS / Piper speaks back  

This pipeline enables **fluid, emotionally aware conversation**.

---

## ğŸ” Privacy by Design

EchoMind is built with privacy as a first-class feature:

- No mandatory cloud calls
- No telemetry
- No tracking
- Memory stored locally
- Models run on your machine

Your data stays **yours**.

---

âš™ï¸ Getting Started
1. Clone the repository
 git clone https://github.com/Ujjwal789/EchoMind.git
cd EchoMind 

2. Set up the environment
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt

3. Launch EchoMind
python main.py

ğŸ“¦ Models & Assets

To keep the repository lightweight, models are not bundled.

Required components

Phi-3 Mini â€” Local LLM

Whisper â€” Speech-to-Text

Piper Voices / Edge TTS â€” Text-to-Speech

Place each model in its respective directory as referenced in the source code.

ğŸ›£ï¸ Product Roadmap

ğŸ”Œ Plugin & skill marketplace

ğŸ–¥ï¸ Desktop UI / tray application

ğŸ§  Smarter long-term memory summarization

ğŸ—£ï¸ Enhanced emotional intelligence

ğŸŒ Cross-platform support

ğŸ‘¥ Multi-user profiles

ğŸ§‘â€ğŸ’» Creator

Built by Ujjwal
Exploring the future of personal, private, human-like AI assistants.


